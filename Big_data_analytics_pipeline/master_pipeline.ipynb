{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae95bdd2-8250-467f-a4b1-bea921a91371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDE80 Master started: 2025-11-12 04:56:30.023222\n✅ UC setup complete\n\n\uD83C\uDFD7️ STEP 1: Ingestion → Bronze\nℹ️ Using existing UC Volume file: dbfs:/Volumes/influencer/raw/ingest_vol/Influencers_dataset.csv\n✅ Bronze table created\n\n\uD83E\uDDF9 STEP 2: Cleaning → Silver\n✅ Silver table created\n\n\uD83E\uDDE0 STEP 3: Feature Engineering + Alignment\n✅ Step 3 complete\n\n\uD83D\uDCA1 STEP 4: Heuristic Success Scoring\n✅ Predictions table created\n\n\uD83D\uDCCA STEP 5: Top Influencer Recommendations\n✅ Top influencers table created\n\n\uD83D\uDD14 STEP 6: Monitoring & Alerts (No Visuals)\n\n\uD83C\uDF89 Pipeline complete: 2025-11-12 04:57:18.423001\n✅ Tables created:\n • influencer.raw.posts_bronze\n • influencer.curated.posts_silver\n • influencer.curated.posts_with_emb\n • influencer.curated.brands_with_emb\n • influencer.curated.post_brand_alignment\n • influencer.ml.creator_features\n • influencer.ml.creator_predictions\n • influencer.curated.top_influencers\n • influencer.curated.kpi_live_summary\n • influencer.curated.alert_summary\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MASTER PIPELINE — Influencer Analytics (All Steps Inline)\n",
    "# Clean Version: No Visualizations, Job-safe, Delta Only\n",
    "# ============================================================\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"\uD83D\uDE80 Master started:\", datetime.now())\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 0: CONFIG + UC OBJECTS\n",
    "# -----------------------------\n",
    "CATALOG      = \"influencer\"\n",
    "SCHEMA_RAW   = \"raw\"\n",
    "SCHEMA_CUR   = \"curated\"\n",
    "SCHEMA_ML    = \"ml\"\n",
    "VOLUME_NAME  = \"ingest_vol\"\n",
    "\n",
    "WORKSPACE_CSV = \"/Workspace/Users/ragutudeepika68730@gmail.com/Big_data_analytics_pipeline/Influencers_dataset.csv\"\n",
    "VOLUME_CSV    = f\"/Volumes/{CATALOG}/{SCHEMA_RAW}/{VOLUME_NAME}/Influencers_dataset.csv\"\n",
    "\n",
    "def tbl(name: str) -> str:\n",
    "    return f\"{CATALOG}.{name}\"\n",
    "\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA  IF NOT EXISTS {CATALOG}.{SCHEMA_RAW}\")\n",
    "spark.sql(f\"CREATE SCHEMA  IF NOT EXISTS {CATALOG}.{SCHEMA_CUR}\")\n",
    "spark.sql(f\"CREATE SCHEMA  IF NOT EXISTS {CATALOG}.{SCHEMA_ML}\")\n",
    "spark.sql(f\"CREATE VOLUME  IF NOT EXISTS {CATALOG}.{SCHEMA_RAW}.{VOLUME_NAME}\")\n",
    "\n",
    "print(\"✅ UC setup complete\")\n",
    "\n",
    "def _exists(dbfs_path: str) -> bool:\n",
    "    try:\n",
    "        dbutils.fs.ls(dbfs_path.rstrip(\"/\"))\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def mirror_to_volume(ws_path: str, vol_path: str) -> str:\n",
    "    vol_dbfs = \"dbfs:\" + vol_path\n",
    "    if _exists(vol_dbfs):\n",
    "        print(f\"ℹ️ Using existing UC Volume file: {vol_dbfs}\")\n",
    "        return vol_path\n",
    "    candidates = []\n",
    "    if ws_path.startswith(\"/Workspace/\"): candidates.append(\"dbfs:\" + ws_path)\n",
    "    if ws_path.startswith(\"dbfs:/\"): candidates.append(ws_path)\n",
    "    candidates.append(\"dbfs:/FileStore/Influencers_dataset.csv\")\n",
    "    for src in candidates:\n",
    "        if not _exists(src): continue\n",
    "        try:\n",
    "            dbutils.fs.cp(src, vol_dbfs)\n",
    "            print(f\"✅ Copied {src} → {vol_dbfs}\")\n",
    "            return vol_path\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise RuntimeError(\n",
    "        f\"❌ Could not copy dataset. Upload manually:\\n{vol_path}\"\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 1: INGEST → BRONZE\n",
    "# -----------------------------\n",
    "print(\"\\n\uD83C\uDFD7️ STEP 1: Ingestion → Bronze\")\n",
    "resolved_csv = mirror_to_volume(WORKSPACE_CSV, VOLUME_CSV)\n",
    "\n",
    "bronze_src = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .option(\"multiLine\", True)\n",
    "         .option(\"mode\", \"PERMISSIVE\")\n",
    "         .option(\"quote\", '\"')\n",
    "         .option(\"escape\", '\"')\n",
    "         .csv(resolved_csv)\n",
    ")\n",
    "\n",
    "cols = set(bronze_src.columns)\n",
    "def pick(cands):\n",
    "    for c in cands:\n",
    "        if c in cols: return F.col(c)\n",
    "    return None\n",
    "def OR(expr, fallback): return expr if expr is not None else fallback\n",
    "\n",
    "PLATFORM=[\"platform\",\"source_platform\",\"source\"]\n",
    "CREATOR =[\"creator_id\",\"user_id\",\"handle\",\"username\",\"channel_id\",\"author_id\"]\n",
    "POSTID  =[\"post_id\",\"content_id\",\"id\",\"tweet_id\",\"video_id\"]\n",
    "TIME    =[\"timestamp\",\"created_at\",\"date\",\"datetime\",\"published_at\"]\n",
    "TEXT    =[\"text\",\"caption\",\"title\",\"body\",\"description\"]\n",
    "LIKE    =[\"like_count\",\"likes\"]\n",
    "COMM    =[\"comment_count\",\"comments\",\"reply_count\"]\n",
    "SHARE   =[\"share_count\",\"shares\",\"retweets\"]\n",
    "COUNTRY =[\"audience_country\",\"country\"]\n",
    "AGE18   =[\"audience_age_18_24\",\"age_18_24\"]\n",
    "AGE25   =[\"audience_age_25_34\",\"age_25_34\"]\n",
    "FEM     =[\"audience_gender_f\",\"female_ratio\"]\n",
    "\n",
    "synthetic_pid = F.concat(F.lit(\"p_\"), F.monotonically_increasing_id())\n",
    "\n",
    "bronze = bronze_src.select(\n",
    "    F.lower(F.trim(OR(pick(PLATFORM), F.lit(\"unknown\")))).alias(\"platform\"),\n",
    "    F.trim(OR(pick(CREATOR), F.lit(\"unknown_creator\"))).alias(\"creator_id\"),\n",
    "    OR(pick(POSTID), synthetic_pid).alias(\"post_id\"),\n",
    "    OR(pick(TIME), F.current_timestamp()).alias(\"timestamp\"),\n",
    "    F.trim(OR(pick(TEXT), F.lit(\"\"))).alias(\"text\"),\n",
    "    F.coalesce(OR(pick(LIKE),  F.lit(None)).cast(\"int\"),   F.lit(0)).alias(\"like_count\"),\n",
    "    F.coalesce(OR(pick(COMM),  F.lit(None)).cast(\"int\"),   F.lit(0)).alias(\"comment_count\"),\n",
    "    F.coalesce(OR(pick(SHARE), F.lit(None)).cast(\"int\"),   F.lit(0)).alias(\"share_count\"),\n",
    "    F.upper(OR(pick(COUNTRY), F.lit(None))).alias(\"audience_country\"),\n",
    "    F.coalesce(OR(pick(AGE18), F.lit(None)).cast(\"double\"),F.lit(0.0)).alias(\"audience_age_18_24\"),\n",
    "    F.coalesce(OR(pick(AGE25), F.lit(None)).cast(\"double\"),F.lit(0.0)).alias(\"audience_age_25_34\"),\n",
    "    F.coalesce(OR(pick(FEM),   F.lit(None)).cast(\"double\"),F.lit(0.0)).alias(\"audience_gender_f\")\n",
    ")\n",
    "(bronze.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\")\n",
    "      .saveAsTable(tbl(f\"{SCHEMA_RAW}.posts_bronze\")))\n",
    "print(\"✅ Bronze table created\")\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 2: CLEAN → SILVER\n",
    "# -----------------------------\n",
    "print(\"\\n\uD83E\uDDF9 STEP 2: Cleaning → Silver\")\n",
    "\n",
    "b = spark.table(tbl(f\"{SCHEMA_RAW}.posts_bronze\"))\n",
    "silver = (\n",
    "    b.withColumn(\"like_count\", F.coalesce(F.col(\"like_count\").cast(\"int\"), F.lit(0)))\n",
    "     .withColumn(\"comment_count\", F.coalesce(F.col(\"comment_count\").cast(\"int\"), F.lit(0)))\n",
    "     .withColumn(\"share_count\", F.coalesce(F.col(\"share_count\").cast(\"int\"), F.lit(0)))\n",
    "     .withColumn(\"audience_age_18_24\", F.coalesce(F.col(\"audience_age_18_24\").cast(\"double\"), F.lit(0.0)))\n",
    "     .withColumn(\"audience_age_25_34\", F.coalesce(F.col(\"audience_age_25_34\").cast(\"double\"), F.lit(0.0)))\n",
    "     .withColumn(\"audience_gender_f\", F.coalesce(F.col(\"audience_gender_f\").cast(\"double\"), F.lit(0.0)))\n",
    "     .withColumn(\"platform\", F.lower(F.trim(\"platform\")))\n",
    "     .withColumn(\"creator_id\", F.trim(\"creator_id\"))\n",
    "     .withColumn(\"post_id\", F.trim(\"post_id\"))\n",
    "     .withColumn(\"text\", F.trim(\"text\"))\n",
    "     .withColumn(\"timestamp\", F.coalesce(F.to_timestamp(\"timestamp\"), F.current_timestamp()))\n",
    "     .filter(\"post_id IS NOT NULL AND creator_id IS NOT NULL AND platform IS NOT NULL\")\n",
    "     .dropDuplicates([\"post_id\"])\n",
    "     .withColumn(\"creator_norm_id\", F.sha2(F.concat_ws(\":\", \"platform\", \"creator_id\"), 256))\n",
    "     .withColumn(\"eng_rate_proxy\", F.col(\"like_count\") + F.col(\"comment_count\") + F.col(\"share_count\"))\n",
    ")\n",
    "(silver.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\")\n",
    "       .saveAsTable(tbl(f\"{SCHEMA_CUR}.posts_silver\")))\n",
    "print(\"✅ Silver table created\")\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 3: FEATURES + ALIGNMENT\n",
    "# -----------------------------\n",
    "print(\"\\n\uD83E\uDDE0 STEP 3: Feature Engineering + Alignment\")\n",
    "\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF\n",
    "\n",
    "posts = spark.table(tbl(f\"{SCHEMA_CUR}.posts_silver\")).fillna({\"text\": \"\"})\n",
    "tok = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens_raw\", pattern=\"\\\\W+\", toLowercase=True)\n",
    "rm  = StopWordsRemover(inputCol=\"tokens_raw\", outputCol=\"tokens\")\n",
    "HASH_DIM = 512\n",
    "tf  = HashingTF(inputCol=\"tokens\", outputCol=\"text_vec\", numFeatures=HASH_DIM, binary=True)\n",
    "posts_vec = tf.transform(rm.transform(tok.transform(posts)))\n",
    "\n",
    "(posts_vec.select(\n",
    "   \"platform\",\"creator_id\",\"creator_norm_id\",\"post_id\",\"timestamp\",\"text\",\n",
    "   \"like_count\",\"comment_count\",\"share_count\",\"audience_country\",\n",
    "   \"audience_age_18_24\",\"audience_age_25_34\",\"audience_gender_f\",\"text_vec\"\n",
    ").write.mode(\"overwrite\").option(\"overwriteSchema\",\"true\")\n",
    " .saveAsTable(tbl(f\"{SCHEMA_CUR}.posts_with_emb\")))\n",
    "\n",
    "brands = spark.createDataFrame([\n",
    "  (\"b_eco_fashion\",\"Sustainable fashion eco friendly materials ethical sourcing\"),\n",
    "  (\"b_gaming\",\"High performance gaming gear esports streaming reviews\"),\n",
    "  (\"b_clean_beauty\",\"Clean beauty cruelty free vegan skincare cosmetics\")\n",
    "], [\"brand_id\",\"text\"])\n",
    "brands_vec = tf.transform(rm.transform(tok.transform(brands))).select(\n",
    "    F.col(\"brand_id\"),\n",
    "    F.col(\"text\").alias(\"brand_desc\"),\n",
    "    F.col(\"text_vec\").alias(\"brand_vec\")\n",
    ")\n",
    "(brands_vec.write.mode(\"overwrite\").option(\"overwriteSchema\",\"true\")\n",
    " .saveAsTable(tbl(f\"{SCHEMA_CUR}.brands_with_emb\")))\n",
    "\n",
    "@F.udf(T.FloatType())\n",
    "def cosine_sim(v1, v2):\n",
    "    if v1 is None or v2 is None: return 0.0\n",
    "    try:\n",
    "        a = v1.toArray(); b = v2.toArray()\n",
    "    except: return 0.0\n",
    "    import numpy as np\n",
    "    na = float(np.linalg.norm(a)); nb = float(np.linalg.norm(b))\n",
    "    return float(np.dot(a,b)/(na*nb)) if na>0 and nb>0 else 0.0\n",
    "\n",
    "p = spark.table(tbl(f\"{SCHEMA_CUR}.posts_with_emb\"))\n",
    "b = spark.table(tbl(f\"{SCHEMA_CUR}.brands_with_emb\"))\n",
    "align = p.crossJoin(b).withColumn(\"content_alignment\", cosine_sim(\"text_vec\",\"brand_vec\"))\n",
    "(align.write.mode(\"overwrite\").option(\"overwriteSchema\",\"true\")\n",
    " .saveAsTable(tbl(f\"{SCHEMA_CUR}.post_brand_alignment\")))\n",
    "\n",
    "# Creator-level features\n",
    "w_creator = Window.partitionBy(\"creator_norm_id\")\n",
    "features = (\n",
    "    posts.withColumn(\"total_posts\", F.count(\"post_id\").over(w_creator))\n",
    "         .withColumn(\"avg_eng_rate\", F.mean(\"eng_rate_proxy\").over(w_creator))\n",
    "         .withColumn(\"avg_like\", F.mean(\"like_count\").over(w_creator))\n",
    "         .withColumn(\"avg_comment\", F.mean(\"comment_count\").over(w_creator))\n",
    "         .withColumn(\"avg_share\", F.mean(\"share_count\").over(w_creator))\n",
    "         .withColumn(\"avg_age_18_24\", F.mean(\"audience_age_18_24\").over(w_creator))\n",
    "         .withColumn(\"avg_age_25_34\", F.mean(\"audience_age_25_34\").over(w_creator))\n",
    "         .withColumn(\"avg_female_ratio\", F.mean(\"audience_gender_f\").over(w_creator))\n",
    "         .withColumn(\"avg_align\", F.when(F.col(\"audience_age_25_34\") > F.col(\"audience_age_18_24\"), 1).otherwise(0))\n",
    "         .withColumn(\"activity_score\", (F.col(\"avg_eng_rate\") / (1 + F.col(\"total_posts\"))))\n",
    "         .dropDuplicates([\"creator_norm_id\"])\n",
    ")\n",
    "(features.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\")\n",
    "         .saveAsTable(tbl(f\"{SCHEMA_ML}.creator_features\")))\n",
    "print(\"✅ Step 3 complete\")\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 4: SUCCESS SCORING\n",
    "# -----------------------------\n",
    "print(\"\\n\uD83D\uDCA1 STEP 4: Heuristic Success Scoring\")\n",
    "\n",
    "df = spark.table(tbl(f\"{SCHEMA_ML}.creator_features\")).fillna(0)\n",
    "feat_cols = [\n",
    "    \"avg_eng_rate\",\"avg_like\",\"avg_comment\",\"avg_share\",\n",
    "    \"avg_female_ratio\",\"activity_score\",\"avg_age_25_34\",\"avg_align\"\n",
    "]\n",
    "eps = 1e-9\n",
    "for c in feat_cols:\n",
    "    q = df.approxQuantile(c, [0.05, 0.95], 1e-3)\n",
    "    lo, hi = q[0], q[1]\n",
    "    if hi - lo <= 0: hi = lo + 1.0\n",
    "    norm = (F.col(c) - F.lit(lo)) / F.lit((hi - lo) + eps)\n",
    "    df = df.withColumn(f\"norm_{c}\", F.when(norm < 0, 0).when(norm > 1, 1).otherwise(norm))\n",
    "\n",
    "weights = {\"avg_eng_rate\":0.3,\"avg_like\":0.1,\"avg_comment\":0.1,\"avg_share\":0.1,\n",
    "           \"avg_female_ratio\":0.05,\"activity_score\":0.15,\"avg_age_25_34\":0.1,\"avg_align\":0.1}\n",
    "\n",
    "expr_sum = None\n",
    "for c, wt in weights.items():\n",
    "    term = F.col(f\"norm_{c}\") * F.lit(wt)\n",
    "    expr_sum = term if expr_sum is None else (expr_sum + term)\n",
    "\n",
    "df = df.withColumn(\"raw_score\", expr_sum)\n",
    "df = df.withColumn(\"success_prob\", 1 / (1 + F.exp(-6 * (F.col(\"raw_score\") - 0.5))))\n",
    "thr = float(df.approxQuantile(\"raw_score\", [0.6], 1e-3)[0])\n",
    "df = df.withColumn(\"label\", (F.col(\"raw_score\") > F.lit(thr)).cast(\"int\"))\n",
    "\n",
    "(df.select(\"creator_norm_id\",\"success_prob\",\"label\")\n",
    "   .write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\")\n",
    "   .saveAsTable(tbl(f\"{SCHEMA_ML}.creator_predictions\")))\n",
    "print(\"✅ Predictions table created\")\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 5: RECOMMENDATIONS\n",
    "# -----------------------------\n",
    "print(\"\\n\uD83D\uDCCA STEP 5: Top Influencer Recommendations\")\n",
    "\n",
    "joined = spark.table(tbl(f\"{SCHEMA_ML}.creator_features\"))\\\n",
    "              .join(spark.table(tbl(f\"{SCHEMA_ML}.creator_predictions\")), \"creator_norm_id\")\n",
    "\n",
    "w_plat = Window.partitionBy(\"platform\").orderBy(F.desc(\"success_prob\"))\n",
    "top10 = (joined\n",
    "         .withColumn(\"rnk\", F.rank().over(w_plat))\n",
    "         .filter(\"rnk <= 10\")\n",
    "         .select(\"platform\",\"creator_id\",\"success_prob\",\"avg_eng_rate\",\"activity_score\",\n",
    "                 \"avg_age_18_24\",\"avg_age_25_34\",\"avg_female_ratio\",\"rnk\"))\n",
    "(top10.write.mode(\"overwrite\").format(\"delta\")\n",
    "      .option(\"overwriteSchema\",\"true\")\n",
    "      .saveAsTable(tbl(f\"{SCHEMA_CUR}.top_influencers\")))\n",
    "print(\"✅ Top influencers table created\")\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 6: MONITORING + ALERTS\n",
    "# -----------------------------\n",
    "print(\"\\n\uD83D\uDD14 STEP 6: Monitoring & Alerts (No Visuals)\")\n",
    "\n",
    "stream_df = joined.withColumn(\"recent_eng_rate\", F.col(\"avg_eng_rate\") * (1 + F.rand()*0.3 - 0.15))\n",
    "agg_df = (stream_df.groupBy(\"platform\")\n",
    "          .agg(F.avg(\"recent_eng_rate\").alias(\"current_eng_rate\"),\n",
    "               F.avg(\"success_prob\").alias(\"avg_success_prob\"),\n",
    "               F.count(\"*\").alias(\"num_creators\"))\n",
    "          .orderBy(F.desc(\"current_eng_rate\")))\n",
    "(agg_df.write.mode(\"overwrite\").format(\"delta\")\n",
    "      .saveAsTable(tbl(f\"{SCHEMA_CUR}.kpi_live_summary\")))\n",
    "\n",
    "alerts = stream_df.withColumn(\n",
    "    \"performance_flag\",\n",
    "    F.when(F.col(\"recent_eng_rate\") < F.col(\"avg_eng_rate\")*0.75, \"DROP\")\n",
    "     .when(F.col(\"recent_eng_rate\") > F.col(\"avg_eng_rate\")*1.25, \"SPIKE\")\n",
    "     .otherwise(\"STABLE\")\n",
    ")\n",
    "(alerts.groupBy(\"platform\",\"performance_flag\").count()\n",
    " .write.mode(\"overwrite\").format(\"delta\")\n",
    " .saveAsTable(tbl(f\"{SCHEMA_CUR}.alert_summary\")))\n",
    "\n",
    "print(\"\\n\uD83C\uDF89 Pipeline complete:\", datetime.now())\n",
    "print(\"✅ Tables created:\")\n",
    "print(f\" • {tbl(f'{SCHEMA_RAW}.posts_bronze')}\")\n",
    "print(f\" • {tbl(f'{SCHEMA_CUR}.posts_silver')}\")\n",
    "print(f\" • {tbl(f'{SCHEMA_CUR}.posts_with_emb')}\")\n",
    "print(f\" • {tbl(f'{SCHEMA_CUR}.brands_with_emb')}\")\n",
    "print(f\" • {tbl(f'{SCHEMA_CUR}.post_brand_alignment')}\")\n",
    "print(f\" • {tbl(f'{SCHEMA_ML}.creator_features')}\")\n",
    "print(f\" • {tbl(f'{SCHEMA_ML}.creator_predictions')}\")\n",
    "print(f\" • {tbl(f'{SCHEMA_CUR}.top_influencers')}\")\n",
    "print(f\" • {tbl(f'{SCHEMA_CUR}.kpi_live_summary')}\")\n",
    "print(f\" • {tbl(f'{SCHEMA_CUR}.alert_summary')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba4d1983-eba3-48f2-ba9b-1d48172a8ed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "master_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}